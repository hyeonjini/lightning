original_work_dir: ${hydar:runtime.cwd}
data_dir: ${original_work_dir}/data/
print_config: true
ignore_warnings: true
train: true
test: false
seed: 42
name: default
datamodule:
  _target_: src.datamodules.cifar100_datamodule.CIFAR100DataModule
  data_dir: ${data_dir}
  batch_size: 64
  train_val_test_split:
  - 55000
  - 5000
  - 10000
  num_workers: 0
  pin_memory: false
model:
  _target_: src.models.cifar100_module.CIFAR100Module
  input_size: 784
  lin1_size: 256
  lin2_size: 256
  lin3_size: 256
  output_size: 10
  lr: 0.001
  weight_decay: 0.0005
callbacks:
  model_checkpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: val/acc
    mode: max
    save_top_k: 1
    save_last: true
    verbose: false
    dirpath: checkpoints/
    filename: epoch_{epoch:03d}
    auto_insert_metric_name: false
  early_stopping:
    _target_: pytorch_lightning.callbacks.EarlyStopping
    monitor: val/acc
    mode: max
    patience: 100
    min_delta: 0
  model_summary:
    _target_: pytorch_lightning.callbacks.RichModelSummary
    max_depth: -1
  rich_progress_bar:
    _target_: pytorch_lightning.callbacks.RichProgressBar
trainer:
  _target_: pytorch_lightning.Trainer
  gpus: 0
  min_epochs: 1
  max_epochs: 10
  resume_from_checkpoint: null
log_dir:
  hydra:
    run:
      dir: logs/experiments/runs/${name}/${now:%Y-%m-%d}_${now:%H-%M-%S}
    sweep:
      dir: logs/experiments/multiruns/${name}/${now:%Y-%m-%d}_${now:%H-%M-%S}
      subdir: ${hydra.job.num}
